#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Created on 2017-08-07 16:11:49
# Project: wanfang_paper_download

from pyspider.libs.base_handler import *
from urllib import request
import requests
import pymysql

class Handler(BaseHandler):
    crawl_config = {
    }

    id = ""
    
    @every(minutes=24 * 60)
    def on_start(self):
        conn= pymysql.connect(host='127.0.0.1',port=3306,user='root',passwd='sdn',db='repository',charset='utf8')
        cur = conn.cursor()
        
        cur.execute("select * from wanfang where paper_flag = 0 limit 300") ;
        rows = cur.fetchall()
        conn.commit()
        cur.close()
        conn.close()
        
        for row in rows:
            self.crawl(row[6], callback=self.index_page , save={'id':row[0],'fileName':row[9][row[9].rindex('/')+1:len(row[9])]})

    @config(age=10 * 24 * 60 * 60)
    def index_page(self, response):
        for each in response.doc('a#doDownload').items():
            fileUrl = each.attr.href
            fileName = "/home/mininet/文档/wanfang/"+response.save['fileName']+".pdf"
            rowId = response.save['id']
            #with request.urlopen(fileUrl) as web:
            #    with open(fileName , 'wb') as outfile:
            #        outfile.write(web.read())
            res = requests.get(fileUrl)
            res.raise_for_status()
            playFile = open(fileName, 'wb')
            for chunk in res.iter_content(100000):
                playFile.write(chunk)
            playFile.close()
            conn= pymysql.connect(host='127.0.0.1',port=3306,user='root',passwd='sdn',db='repository',charset='utf8')
            cur = conn.cursor()
            cur.execute("update wanfang set file_path = '%s' , paper_flag = 1 where id = %s" % (fileName , int(rowId)))
            conn.commit()
            cur.close()
            conn.close()

    @config(priority=2)
    def detail_page(self, response):
        return {
            "result" : "ok"
        }



